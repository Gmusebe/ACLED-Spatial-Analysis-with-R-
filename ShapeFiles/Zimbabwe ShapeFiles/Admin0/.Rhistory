install.packages(c("rtweet", "tidyverse", "tidytext"))
library(rtweet)
library(tidyverse)
library(tidytext)
API_Key <- "9JDmpjYlbTDelVDjAvC83eJBH"
API_Secret_Key <- "INN7ZWx5hFWVggLmNekXPNVhR9iNXnwhzGZvOwyAJG3Fr4rTsI"
Bearer_Token <- "AAAAAAAAAAAAAAAAAAAAAKnqQQEAAAAANvWhE74DwzQqTyusbSzvbZ1h60o%3DBbYAHh30S6GuUzoRs2GU6vJvwCwnKVmBWQUswgIk6cKTkEOE19"
setup_twitter_oauth(API_Key, API_Secret_Key, Bearer_Token)
install.packages("twitteR")
library(twitteR)
setup_twitter_oauth(API_Key, API_Secret_Key, Bearer_Token)
Access_Token <- "1263742897154818049-dGT6nKdbX8XPCTtRcBIsDVOuKsrcJb"
Access_Token_Secret <-"Vko3wJIuEicPYv552eA7ocomjcxavlsxEiNj0YhMQejVx"
setup_twitter_oauth(API_Key, API_Secret_Key, Access_Token, Access_Token_Secret)
post_tweet(""Tweeting from R in my #rstats #Twitteranalytics research!")
library(rtweet)
post_tweet(""Tweeting from R in my #rstats #Twitteranalytics research!")
post_tweet("Tweeting from R in my #rstats #Twitteranalytics research!")
?searchTwitter
security_info <- searchTwitter('#rape, #murder, #killing, #violence, #robbery,
#theft, #criminal, #fraud, #police, #crime, #protests',
lang = "en",
geocode = '-28.48322,24.676997, 2978km'
n = 2000, since = '2020-01-01', retryOnRateLimit = 1e3)
security_info_data = twListToDF(security_info)
security_info <- searchTwitter('#rape, #murder, #killing, #violence, #robbery,
#theft, #criminal, #fraud, #police, #crime, #protests',
lang = "en",
geocode = '-28.48322,24.676997, 2978km'
n = 2000, since = '2020-01-01', retryOnRateLimit = 1e3)
security_info <- searchTwitter('#rape, #murder, #killing, #violence, #robbery,
#theft, #criminal, #fraud, #police, #crime, #protests',
lang = "en",
geocode = '-28.48322,24.676997, 2978km',
n = 2000, since = '2020-01-01', retryOnRateLimit = 1e3)
tw = twitteR::searchTwitter('#realDonaldTrump + #HillaryClinton', n = 5, since = '2016-11-08', retryOnRateLimit = 1e3)
d = twitteR::twListToDF(tw)
security_info <- searchTwitter('#rape, #murder, #killing, #violence, #robbery,
#theft, #criminal, #fraud, #police, #crime, #protests',
lang = "en",
geocode = '-28.48322,24.676997, 2978km')
listTA <- searchTwitter('#protest')
security_info <- searchTwitter(c('#rape', '#murder', '#killing', '#violence', '#robbery',
'#theft', '#criminal', '#fraud', '#police', '#crime', '#protests'),
lang = "en",
geocode = '-28.48322,24.676997, 2978km',
n = 2000, since = '2020-01-01', retryOnRateLimit = 1e3)
hashtags <- c('#rape', '#murder', '#killing', '#violence', '#robbery',
'#theft', '#criminal', '#fraud', '#police', '#crime', '#protests')
needle <- paste(hashtags, collapse = " OR ")
security_info <- searchTwitter(needle,
lang = "en",
geocode = '-28.48322,24.676997, 2978km',
n = 2000, since = '2020-01-01', retryOnRateLimit = 1e3)
security_info <- searchTwitter(needle,
lang = "en",
geocode = '-28.48322,24.676997, 2978km')
security_info <- searchTwitter(needle,
lang = "en")
security_info_data = twListToDF(security_info)
View(security_info_data)
security_info <- searchTwitter(needle,
lang = "en", n = 1000)
security_info_data = twListToDF(security_info)
View(security_info_data)
security_info <- searchTwitter(needle,
lang = "en", n = 1000,
geocode = lookup_coords("usa"))
security_info <- searchTwitter(needle,
lang = "en", n = 1000)
security_info.df = do.call("rbind",lapply(security_info,as.data.frame))
View(security_info.df)
security_info <- searchTwitter(needle,
lang = "en", n = 1000,
geocode='42.375,-71.1061111,10mi')
security_info_data = twListToDF(security_info)
View(security_info_data)
security_info <- searchTwitter(needle,
lang = "en", n = 25,
geocode='42.375,-71.1061111,10mi')
security_info_data = twListToDF(security_info)
users <- lookupUsers(security_info_data$screenName)
users_df <- twListToDF(users)
table(users_df[, 'location'])
x <- table(users_df[, 'location'])
x <- data.frame(table(users_df[, 'location']))
View(x)
security_info <- searchTwitter(needle,
lang = "en", n = 25,
geocode("London", source = "dsk"))
security_info <- searchTwitter(needle,
lang = "en", n = 25,
geocode = geocode("London", source = "dsk"))
security_info <- search_tweets(hashtags,
n = 1000,
geocode = geocode("London", source = "dsk"),
include_rts = FALSE)
security_info <- search_tweets(hashtags,
n = 1000,
include_rts = FALSE)
security_info <- search_tweets(needle,
n = 1000,
include_rts = FALSE)
View(security_info)
library(stringr)
library(ggmap)
user_info <- lookup_users(unique(security_info$user_id))
str(security_info)
discard(user_info$location, `==`, "") %>%
ggmap::geocode() -> coded
library(RgoogleMaps)
register_google(key = "AIzaSyDLgK2Ld_sClr5PZgBxJYhlfq_4nXCaVzc")
discard(user_info$location, `==`, "") %>%
ggmap::geocode() -> coded
security_info <- search_tweets(needle,
n = 30,
include_rts = FALSE)
user_info <- lookup_users(unique(security_info$user_id))
discard(user_info$location, `==`, "") %>%
ggmap::geocode() -> coded
coded$location <- discard(user_info$location, `==`, "")
user_info <- left_join(user_info, coded, "location")
View(user_info)
View(coded)
security_info <- search_tweets(needle,
n = 100,
include_rts = FALSE,
geocode = lookup_coords("usa"))
security_info <- lat_lng(security_info)
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)
install.packages('maps')
View(security_info)
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)
with(security_info, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
security_info <- search_tweets(needle,
n = 500,
include_rts = FALSE,
geocode = lookup_coords("usa"))
security_info <- lat_lng(security_info)
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)
with(security_info, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
library(RgoogleMaps)
library(tidyverse)
library(tidytext)
library(twitteR)
library(stringr)
library(ggplot2)
library(leaflet)
library(rtweet)
library(ggmap)
library(dplyr)
library(plyr)
library(maps)
library(sf)
# Link Twitter Application and R.
# Store Keys and Tokens into variables:
API_Key <- "9JDmpjYlbTDelVDjAvC83eJBH"
API_Secret_Key <- "INN7ZWx5hFWVggLmNekXPNVhR9iNXnwhzGZvOwyAJG3Fr4rTsI"
Bearer_Token <- "AAAAAAAAAAAAAAAAAAAAAKnqQQEAAAAANvWhE74DwzQqTyusbSzvbZ1h60o%3DBbYAHh30S6GuUzoRs2GU6vJvwCwnKVmBWQUswgIk6cKTkEOE19"
Access_Token <- "1263742897154818049-dGT6nKdbX8XPCTtRcBIsDVOuKsrcJb"
Access_Token_Secret <-"Vko3wJIuEicPYv552eA7ocomjcxavlsxEiNj0YhMQejVx"
# Set Google API Key:
register_google(key = "AIzaSyDLgK2Ld_sClr5PZgBxJYhlfq_4nXCaVzc")
# Setup Authentication
setup_twitter_oauth(API_Key, API_Secret_Key, Access_Token, Access_Token_Secret)
# Search and extract crime data:
hashtags <- c('#rape', '#murder', '#killing', '#violence', '#robbery',
'#theft', '#criminal', '#fraud', '#police', '#crime', '#protests', 'protest',
'#civilunrest', '#burglary', 'war', 'hatecrime', 'criminaljustice', '#murder',
'#serialkiller', '#serialkillers', '#justice', '#death', '#terrorism', '#terrorist',
'#terroristattack', '#violence', '#isis')
needle <- paste(hashtags, collapse = " OR ")
security_info <- search_tweets(needle,
n = 50000,
include_rts = FALSE,
geocode = lookup_coords("usa"),
retryonratelimit = TRUE)
security_info <- search_tweets(needle,
n = 50000,
include_rts = FALSE,
geocode = lookup_coords("usa"),
retryonratelimit = TRUE)
library(RgoogleMaps)
library(tidyverse)
library(tidytext)
library(twitteR)
library(stringr)
library(ggplot2)
library(leaflet)
library(rtweet)
library(ggmap)
library(dplyr)
library(plyr)
library(maps)
library(sf)
# Store Keys and Tokens into variables:
API_Key <- "9JDmpjYlbTDelVDjAvC83eJBH"
API_Secret_Key <- "INN7ZWx5hFWVggLmNekXPNVhR9iNXnwhzGZvOwyAJG3Fr4rTsI"
Bearer_Token <- "AAAAAAAAAAAAAAAAAAAAAKnqQQEAAAAANvWhE74DwzQqTyusbSzvbZ1h60o%3DBbYAHh30S6GuUzoRs2GU6vJvwCwnKVmBWQUswgIk6cKTkEOE19"
Access_Token <- "1263742897154818049-dGT6nKdbX8XPCTtRcBIsDVOuKsrcJb"
Access_Token_Secret <-"Vko3wJIuEicPYv552eA7ocomjcxavlsxEiNj0YhMQejVx"
register_google(key = "AIzaSyDLgK2Ld_sClr5PZgBxJYhlfq_4nXCaVzc")
setup_twitter_oauth(API_Key, API_Secret_Key, Access_Token, Access_Token_Secret)
hashtags <- c('#rape', '#murder', '#killing', '#violence', '#robbery',
'#theft', '#criminal', '#fraud', '#police', '#crime', '#protests', 'protest',
'#civilunrest', '#burglary', 'war', 'hatecrime', 'criminaljustice', '#murder',
'#serialkiller', '#serialkillers', '#justice', '#death', '#terrorism', '#terrorist',
'#terroristattack', '#violence', '#isis')
needle <- paste(hashtags, collapse = " OR ")
security_info <- search_tweets(needle,
n = 50000,
include_rts = FALSE,
geocode = lookup_coords("usa"),
retryonratelimit = TRUE)
library(wordcloud)
library(RColorBrewer)
hashtags <-data.frame(table(unlist(security_info$hashtags)))
hashtags <- hashtags[which(hashtags$Freq > 4),]
related_hashtags <- c("abuse", "AlQaeda", "arrests", "assault", "bomb", "cannabis", "ChildSexTrafficking",
"CIA", "CivilWar", "ClimateCrisis", "coldwar", "ColdWar", "corruption", "crime", "Crime",
"CrimesAgainstHumanity", "CrimeStoppers", "criminal", "Criminal", "CRIMINAL",
"criminaljustice", "CriminalJustice", "CultureWar", "cyberattacks", "cybercrime", "CyberCrime",
"cybersecurity", "CyberSecurity", "cyberthreats", "danger", "Death", "death", "dead","DrugCartel", "drugs",
"Embezzlement", "EndProxyWar", "EndSARS", "FBI", "Federal", "federal", "Fire", "FIRE", "fire",
"FRAUD", "guns", "hatecrime", "HumanTrafficking", "insurancefraud", "ISIS", "justice",
"Justice", "killed", "murder", "Murder", "PoliceBrutality", "protest", "Protest", "protests", "Protests",
"racism", "racist", "Rape", "rape", "robbery", "SerialKiller",
"serialKiller", "serialKillers", "shooting", "Shooting", "terrorism", "Terrorism", "terrorist",
"Terrorist")
hashtags = hashtags[which(hashtags$Var1 %in% related_hashtags),]
hashtags <- hashtags %>%
mutate(Var1 = tolower(Var1)) %>% ddply("Var1", numcolwise(sum))
wordcloud(word = hashtags$Var1, freq = hashtags$Freq, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
security_info %>%
filter(is.na(place_full_name) == FALSE & place_full_name != "") %>%
dplyr::count(place_full_name, sort = TRUE) %>%
slice(1:10)
security_info <- lat_lng(security_info)
security_info_geo <- lat_lng(security_info) %>%
filter(is.na(lat) == FALSE & is.na(lng) == FALSE)
security_info_geo.sf <- st_as_sf(security_info_geo , coords = c("lng", "lat"), crs = "+proj=longlat +datum=WGS84 +ellps=WGS84")
leaflet() %>%
addProviderTiles("OpenStreetMap.Mapnik") %>%
addCircles(data = security_info_geo.sf,
color = "blue")
library(tm)
security_tweets <- security_info$text
corp <- Corpus(VectorSource(security_tweets))
# Custom functions
twitterHandleRemover <- function(x) gsub("@\\S+","", x)
hashtagRemover <- function(x) gsub("#\\S+","", x)
emojiRemover <- function(x) gsub("[^\x01-\x74F]","",x)
toSpace = content_transformer(function(x,pattern)gsub(pattern,"",x))
cleaner <- function(corp){
corp <- tm_map(corp, toSpace," ?(f|ht)tp(s?)://(.*)[.][a-z]+")
corp <- tm_map(corp, content_transformer(twitterHandleRemover))
corp <- tm_map(corp, content_transformer(hashtagRemover))
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, emojiRemover)
corp <- tm_map(corp, stemDocument)
corp <- tm_map(corp, content_transformer(tolower))
return(corp)
}
corp <- cleaner(corp)
View(corp)
new_tweetsdf <- data.frame(text = sapply(corp, as.character), stringsAsFactors = FALSE)
View(new_tweetsdf)
dtm <- DocumentTermMatrix(corp)
dtm
sparseData <- removeSparseTerms(dtm, sparse=0.995)
sparseData
sparsedf <- as.data.frame(as.matrix(sparseData))
head(sparsedf[,1:6])
install.packages("RTextTools")
library(RTextTools)
set.seed(16102016)
samp_id = sample(1:nrow(security_info),              # do ?sample to examine the sample() func
round(nrow(data)*.70),     # 70% records will be used for training
replace = F)
library(tm)
samp_id = sample(1:nrow(security_info),
round(nrow(security_info)*.70),
replace = F)
train = security_info[samp_id,]
test = security_info[-samp_id,]
train.data = rbind(train,test)
train.data$text = tolower(train.data$text)
security_tweets <- train.data$text
corp <- Corpus(VectorSource(security_tweets))
# Custom functions
twitterHandleRemover <- function(x) gsub("@\\S+","", x)
hashtagRemover <- function(x) gsub("#\\S+","", x)
emojiRemover <- function(x) gsub("[^\x01-\x74F]","",x)
toSpace = content_transformer(function(x,pattern)gsub(pattern,"",x))
cleaner <- function(corp){
corp <- tm_map(corp, toSpace," ?(f|ht)tp(s?)://(.*)[.][a-z]+")
corp <- tm_map(corp, content_transformer(twitterHandleRemover))
corp <- tm_map(corp, content_transformer(hashtagRemover))
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, emojiRemover)
corp <- tm_map(corp, stemDocument)
corp <- tm_map(corp, content_transformer(tolower))
return(corp)
}
corp <- cleaner(corp)
dtm = DocumentTermMatrix(corp,
control = list(weighting =
function(x)
weightTfIdf(x, normalize = F)))
training_codes = train.data$Classification
str(train.data)
View(train.data)
View(dtm)
library(tm)
security_tweets <- security_info$text
corp <- Corpus(VectorSource(security_tweets))
cleaner <- function(corp){
corp <- tm_map(corp, toSpace," ?(f|ht)tp(s?)://(.*)[.][a-z]+")
corp <- tm_map(corp, content_transformer(twitterHandleRemover))
corp <- tm_map(corp, content_transformer(hashtagRemover))
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, emojiRemover)
corp <- tm_map(corp, stemDocument)
corp <- tm_map(corp, content_transformer(tolower))
return(corp)
}
corp <- cleaner(corp)
new_tweetsdf <- data.frame(text = sapply(corp, as.character), stringsAsFactors = FALSE)
View(new_tweetsdf)
new_tweetsdf <- unlist(new_tweetsdf)
sum(!complete.cases(new_tweetsdf))
sentiments_df <- sentiment_attributes(new_tweetsdf)
library(exploratory)
library(sentimentr)
library(devtools)
library(gofastr)
library(plotly)
sentiments_df <- sentiment_attributes(new_tweetsdf)
new_2 <- get_sentences(new_tweetsdf)
tweet_sentiment<-sentiment_by(new_2, averaging.function = average_weighted_mixed_sentiment)
View(tweet_sentiment)
security_info$sent_score = tweet_sentiment$ave_sentiment
security_info$Negative <- as.factor(security_info$sent_score <= -1)
View(security_info)
head(security_info$Negative)
table(security_info$Negative)
security_info$Positive = as.factor(security_info$sent_score >= 1)
security_tweets <- security_info$text
corp <- Corpus(VectorSource(security_tweets))
# Custom functions
twitterHandleRemover <- function(x) gsub("@\\S+","", x)
hashtagRemover <- function(x) gsub("#\\S+","", x)
emojiRemover <- function(x) gsub("[^\x01-\x74F]","",x)
toSpace = content_transformer(function(x,pattern)gsub(pattern,"",x))
cleaner <- function(corp){
corp <- tm_map(corp, toSpace," ?(f|ht)tp(s?)://(.*)[.][a-z]+")
corp <- tm_map(corp, content_transformer(twitterHandleRemover))
corp <- tm_map(corp, content_transformer(hashtagRemover))
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, emojiRemover)
corp <- tm_map(corp, stemDocument)
corp <- tm_map(corp, content_transformer(tolower))
return(corp)
}
corp <- cleaner(corp)
corp <- tm_map(corp, stemDocument)
corp[[1]]
corp[[50]]
dtm = DocumentTermMatrix(corp,
control = list(weighting =
function(x)
weightTfIdf(x, normalize = F)))
dtm = DocumentTermMatrix(corp)
dtm
inspect(dtm[1000:1005, 505:515])
sparse_dtm <- removeSparseTerms(dtm, 0.995)
sparse_dtm
dtm_df <- as.data.frame(as.matrix(sparse_dtm))
View(dtm_df)
dtm_df$Negative <- security_info$Negative
dtm_df$Positive <- security_info$Positive
samp_id = sample.split(dtm_df$Negative, splitRatio = 0.7)
install.packages("caTools")
install.packages("caTools")
library(caTools)
library(caTools)
set.seed(16102016)
samp_id = sample.split(dtm_df$Negative, splitRatio = 0.7)
samp_id = sample(1:nrow(dtm_df),
round(nrow(dtm_df)*.70),
replace = F)
train = security_info[samp_id,]
test = security_info[-samp_id,]
install.packages("rpart.plot")
library(rpart.plot)
install.packages("rpart")
install.packages("rpart")
library(rpart)
npm <- rpart(Negative ~ . , data = train, method = "class")
npm <- rpart(train$Negative ~ . , data = train, method = "class")
View(train)
train1 <- train[, c("Positive", "Negative", "text", "sent_score")]
View(train1)
npm <- rpart(Negative ~ . , data = train1, method = "class")
prp(npm)
ppm <- rpart(Positive ~ . , data = train1, method = "class")
predict_positive <- predict(ppm, newdata = test, type = "class")
View(dtm_df)
dtm_df <- dtm_df[, c("Positive", "Negative", "text", "sent_score")]
negative_split <- sample.split(dtm_df$Negative, SplitRatio = 0.7)
negative_train <- subset(dtm_df, negative_split == TRUE)
View(negative_train)
negative_split <- sample.split(dtm_df$Negative, SplitRatio = 0.7)
negative_train <- subset(dtm_df, negative_split == TRUE)
negative_test <- subset(dtm_df, negative_split == FALSE)
positive_split <- sample.split(dtm_df$Negative, SplitRatio = 0.7)
positive_train <- subset(dtm_df, positive_split == TRUE)
positive_test <- subset(dtm_df, positive_split == FALSE)
npm <- rpart(Negative ~ . , data = negative_train, method = "class")
ppm <- rpart(Positive ~ . , data = positive_train, method = "class")
prp(ppm)
prp(npm)
predict_negatibe <- predict(npm, newdata = negative_test, type = "class")
predict_positive <- predict(ppm, newdata = positive_test, type = "class")
table(negative_split$Negative , predict_negative)
View(positive_test)
table(negative_test$Negative , predict_negative)
table(negative_test$Negative , predict_negatibe)
ng_ac <-  table(negative_test$Negative , predict_negative)
ng_ac <-  table(negative_test$Negative , predict_negatibe)
(ng_ac[1,1] + ng_ac[2,2])/sum(ng_ac)
negative_predictive_acc <- (ng_ac[1,1] + ng_ac[2,2])/sum(ng_ac)
negative_predictive_acc
pg_ac <-  table(positive_test$Positive , predict_positive)
table(positive_test$Positive , predict_positive)
(pg_ac[1,1] + pg_ac[2,2])/sum(pg_ac)
positive_predictive_acc <- (pg_ac[1,1] + pg_ac[2,2])/sum(pg_ac)
cmat_baseline <- table(negative_test$Negative)
cmat_baseline
accu_baseline <- max(cmat_baseline)/sum(cmat_baseline)
accu_baseline
cmat_baselineP <- table(positive_test$Positive)
cmat_baselineP
library(rtweet)
library(tidyverse)
library(twitteR)
API_Key <- "9JDmpjYlbTDelVDjAvC83eJBH"
API_Secret_Key <- "INN7ZWx5hFWVggLmNekXPNVhR9iNXnwhzGZvOwyAJG3Fr4rTsI"
Bearer_Token <- "AAAAAAAAAAAAAAAAAAAAAKnqQQEAAAAANvWhE74DwzQqTyusbSzvbZ1h60o%3DBbYAHh30S6GuUzoRs2GU6vJvwCwnKVmBWQUswgIk6cKTkEOE19"
Access_Token <- "1263742897154818049-dGT6nKdbX8XPCTtRcBIsDVOuKsrcJb"
Access_Token_Secret <-"Vko3wJIuEicPYv552eA7ocomjcxavlsxEiNj0YhMQejVx"
setup_twitter_oauth(API_Key, API_Secret_Key, Access_Token, Access_Token_Secret)
tweets <- userTimeline("RDataMining", n = 3200)
(n.tweet <- length(tweets))
tweets.df <- twListToDF(tweets)
tweets.df[190, c("id", "created", "screenName", "replyToSN",
"favoriteCount", "retweetCount", "longitude", "latitude", "text")]
writeLines(strwrap(tweets.df$text[190], 60))
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),
"use", "see", "used", "via", "amp")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
writeLines(strwrap(myCorpus[[190]]$content, 60))
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
writeLines(strwrap(myCorpus[[190]]$content, 60))
library(tidyverse)
library(lubridate)
library(RgoogleMaps)
library(RColorBrewer)
library(stringr)
library(leaflet)
library(reshape2)
library(scales)
library(magrittr)
library(tibble)
library(naniar)
library(ggpubr)
library(plotrix)
library(ggmap)
library(stringi)
library(dplyr)
library(date)
library(getPass)
library(RMariaDB)
library(tidyr)
library(RMySQL)
library(rgdal)
setwd("~/Downloads/Zimbabwe ShapeFiles")
path <- "/Admin0/zwe_admbnda_adm0_zimstat_ocha_20180911.shp"
zim <- rgdal::readOGR(path)
path <- "Admin0/zwe_admbnda_adm0_zimstat_ocha_20180911.shp"
zim <- rgdal::readOGR(path)
zim <- st_read(path, stringsAsFactors = FALSE)
library(sf)
zim <- st_read(path, stringsAsFactors = FALSE)
path <- "/Admin0/zwe_admbnda_adm0_zimstat_ocha_20180911.shp"
zim <- st_read(path, stringsAsFactors = FALSE)
setwd("~/Downloads/Zimbabwe ShapeFiles/Admin0")
setwd("~/Downloads/Zimbabwe ShapeFiles/Admin0")
